---
title: "Rhine level prediction"
output:
  html_document:
    toc: true
    df_print: paged
---

In this notebook we present the full prediction pipeline, including data load, transformation, modelling and evaluation.

This solution uses R and a couple of external packages to make the code concise.

```{r message=FALSE, warning=FALSE}
#Package Dependencies
library(data.table) # Using it for fast and data transformation
library(ggplot2) # To display nice plots
library(zoo) # To manipulate time series and rolling averages
library(GGally) # For nice scatter plots
library(neuralnet) # For neural netwrk models
library(rpart) # Decision tree models
library(rpart.plot) # Tree Plot utility
library(randomForest) # For random forest models
#options(max.print=400)
```

# Data Load

Let's see what data do we have, how many stations are available for the analysis.

```{r}
files.in.folder <- dir("data/raw", full.names = T)
print(files.in.folder)
```

We can also check how the files are structured and how we can load them into R 

```{r}
file.type1 <- "data/raw/Düsseldorf Q15.zrx"
file.type2 <- "data/raw/Düsseldorf W15.zrx"
file.type3 <- "data/raw/BONN_produkt_rr_stunde_20170504_20181104_00603.txt"
file.type4 <- "data/raw/BONN_produkt_tu_stunde_20170504_20181104_00603.txt"
cat(paste("\n",file.type1, "\n"))
readLines(file.type1, n = 10)
cat(paste("\n",file.type2, "\n"))
readLines(file.type2, n = 10)
cat(paste("\n",file.type3, "\n"))
readLines(file.type3, n = 10)
cat(paste("\n",file.type4, "\n"))
readLines(file.type4, n = 10)
```

The text files give us many hints, like the field separator (tab or semicolon), the columns available and whether the
files have header or not

```{r}
dataset <- read.table(file.type1, sep=" ")
head(dataset)
dataset2 <- read.table(file.type2, sep =" ")
head(dataset2)
dataset3 <- read.table(file.type3, sep =";", header = T)
head(dataset3)
dataset4 <- read.table(file.type4, sep =";", header = T)
head(dataset4)

plot(dataset$V2, type = "p", ylab = "Level", main = "Water Level in Düsseldorf")

```


```{r}
#Lets filter the existing files, according to their patterns
files.temp <- grep("tu_stunde", files.in.folder, value = T)
files.rain <- grep("rr_stunde", files.in.folder, value = T)
files.flow <- grep("Q15", files.in.folder, value = T)
files.level <- grep("W15", files.in.folder, value = T)
```

## Batch data load

We can repeat the process for the rest files and merge all the similar data sets, if we add a "STATION" column we will be able to differentiate them.

For that we first build a function that loads a list of files and then merges them into one data.frame (in this case a data.table).


```{r}
read.river.data <- function(filenames = NULL, separator = " ", headers = F){
  contents <- lapply(filenames, function(datafile) {
    dt <- read.table(datafile, header = headers, sep = separator, encoding = "UTF-8", na.strings = "NULL")
    dt$STATION <- as.factor(toupper(strsplit(basename(datafile), split = "( |_|-)")[[1]][1]))
    dt
  })
  data.table::rbindlist(contents, use.names = F)
}

```

We provide different parameters since the field separators are not equal for all the file types.

After reading the files we reinterpret (cast) the timestamps and fix the station names.


```{r}
level.data <- read.river.data(files.level)
level.data <- level.data[, .(TIMESTAMP = strptime(format(V1, scientific = F), "%Y%m%d%H%M%S", tz = "GMT"),
                             VALUE = V2, STATION, KPI = as.factor("LEVEL"))]
level.data[STATION == "DÜSSELDORF", STATION := "DUESSELDORF"]
level.data[STATION == "MANNHEIMNECKAR", STATION := "MANNHEIM"]
level.data$STATION <- droplevels(level.data$STATION)

summary(level.data)
```

```{r}
flow.data <- read.river.data(files.flow)
flow.data <- flow.data[, .(TIMESTAMP = strptime(format(V1, scientific = F), "%Y%m%d%H%M%S", tz = "GMT"),
                           VALUE = V2, STATION, KPI = as.factor("FLOW"))]
flow.data[STATION=="DÜSSELDORF", STATION := "DUESSELDORF"]
flow.data$STATION <- droplevels(flow.data$STATION)
summary(flow.data)
```

```{r}

temp.data <- read.river.data(files.temp, separator = ";", headers = T)
temp.data <- temp.data[, .(TIMESTAMP = strptime(format(MESS_DATUM, scientific = F), "%Y%m%d%H", tz = "GMT"),
                           VALUE = TT_TU, STATION, KPI = as.factor("TEMPERATURE"))]
temp.data[STATION=="DÜDO", STATION := "DUESSELDORF"]
temp.data$STATION <- droplevels(temp.data$STATION)
summary(temp.data)
```

```{r}
rain.data <- read.river.data(files.rain, separator = ";", headers = T)
rain.data <- rain.data[, .(TIMESTAMP = strptime(format(MESS_DATUM, scientific = F), "%Y%m%d%H", tz = "GMT"),
                           VALUE = R1, STATION, KPI = "RAIN")]
rain.data[STATION=="DÜDO", STATION := "DUESSELDORF"]
rain.data$STATION <- droplevels(rain.data$STATION)
summary(rain.data)
```


The summary for each data.frame gives us the information about data ranges, missing values and column names. Easily we can detect that some of the values are not valid. We will then proceed to clean the data.

# Data cleansing and transformation

Let's start correcting the level data. Here many values are under zero and other are outliers. A good option is to replace the missing values with the last one that was correct.

```{r}
level.data.clean <- rbindlist(lapply(split(level.data, by = "STATION"), function(dt){
  dt[, VALUE := VALUE[1], .(cumsum(VALUE>=0))]
  dt[, VALUE := VALUE[1], .(cumsum(VALUE<=1000))]
  dt
}))

```

We do something similar for the flow data

```{r}
flow.data.clean <- rbindlist(lapply(split(flow.data, by = "STATION"), function(dt){
  dt[, VALUE := VALUE[1], .(cumsum(VALUE>=0))]
  dt
}))

```


For the negative rain data, we replace the negative values by zero.

```{r}
rain.data.clean <- copy(rain.data)
rain.data.clean[VALUE<0, VALUE := 0]

rain.data.acc <- rbindlist(lapply(split(rain.data.clean, by = "STATION"), function(dt){
  dt[, ROLL_VALUE := rollmean(VALUE, 24, fill = 0)]
  dt[, .(TIMESTAMP, VALUE = ROLL_VALUE, STATION, KPI = as.factor("RAIN24H"))]
}))

```

Negative temperature is possible, but up to a limit. We correct this data too

```{r}
#Replace temperature outliers
temp.data.clean <- rbindlist(lapply(split(temp.data, by = "STATION"), function(dt){
  dt[, VALUE := VALUE[1], .(cumsum(VALUE>=-20))]
}))

temp.data.acc <- rbindlist(lapply(split(temp.data, by = "STATION"), function(dt){
  dt[, VALUE := VALUE[1], .(cumsum(VALUE>=-20))]
  dt[, ROLL_VALUE := rollmean(VALUE, 24, fill = 0)]
  dt[, .(TIMESTAMP, VALUE = ROLL_VALUE, STATION, KPI = as.factor("TEMP24H"))]
}))
```




```{r}
mints <- max(min(rain.data.acc$TIMESTAMP),
             min(flow.data.clean$TIMESTAMP),
             min(rain.data.acc$TIMESTAMP),
             min(temp.data.acc$TIMESTAMP))
maxts <- min(max(level.data.clean$TIMESTAMP),
             max(flow.data.clean$TIMESTAMP),
             max(rain.data.acc$TIMESTAMP),
             max(temp.data.acc$TIMESTAMP))
full.data <- rbindlist(list(rain.data.acc[TIMESTAMP >= mints & TIMESTAMP <= maxts, ],
                            temp.data.acc[TIMESTAMP >= mints & TIMESTAMP <= maxts, ],
                            flow.data.clean[format(TIMESTAMP, "%M") == "00"
                                            & TIMESTAMP >= mints & TIMESTAMP <= maxts, ],
                            level.data.clean[format(TIMESTAMP, "%M") == "00"
                                             & TIMESTAMP >= mints & TIMESTAMP <= maxts, ]),
                       use.names = T, fill = T)
```



# Data exploration

Let's see how the water level evolves in  several regions

```{r}
ggplot(data = level.data.clean[KPI == "LEVEL" 
                        & STATION == "DUESSELDORF",],
       aes(x=TIMESTAMP, y=VALUE)) + 
  geom_line(aes(color=STATION), show.legend = F) +
  labs(y = "River level [cm]",
       x = "Day",
       colour = "Meassuring Station",
       title = "River Level Evolution in Düsseldorf")
```


```{r}
ggplot(data = level.data.clean[KPI == "LEVEL" & 
                                 STATION %in% c("DUESSELDORF", "BONN", "MANNHEIM", "KOBLENZ", "HEIDELBERG"), ],
       aes(x=TIMESTAMP, y=VALUE)) + 
  geom_line(aes(color=STATION)) +
  labs(y = "River level [cm]",
       x = "Day",
       colour = "Meassuring Station",
       title = "River Level Evolution")
```

The last plot shows certain relation between these meassuring points. What about the numeric correlation between them and the other factors?

## Factor Analysis

In order to analyze the different factors that affect the water level in Duesseldorf we will first
transform the data.frame to a wide format and select some of the columns.

```{r}

wide.data <- dcast(full.data, TIMESTAMP ~ STATION + KPI, value.var = "VALUE", fill = 0)
write.csv2(wide.data, file = "datawide_m1.txt", row.names = F)
summary(wide.data)

```
